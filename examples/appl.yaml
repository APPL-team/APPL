settings:
  logging:
    # log_file:
    #   enabled: true
    display:
      configs: false
      llm_call_args: false # true
      llm_response: false # true
      llm_cache: false
      llm_cost: true # true
  tracing:
    enabled: true

# example for setting up servers
servers:
  # default: azure-gpt35 # override the default server according to your needs
  azure-gpt35: # the name of the server
    model: azure/reap-gpt35-1106 # the model name
    # temperature: 1.0 # set the default temperature for the calls to this server
  gpt4-preview:
    model: gpt-4-0125-preview
  moonshot-8k:
    model: moonshot-v1-8k
    provider: api
    is_custom_llm: true # https://docs.litellm.ai/docs/providers/custom_openai_proxy
    api_key: os.environ/MOONSHOT_API_KEY # setup your API key in the environment variable
    base_url: "https://api.moonshot.cn/v1"
    # add cost, see also https://docs.litellm.ai/docs/proxy/custom_pricing
    input_cost_per_token: 1.2e-5
    output_cost_per_token: 1.2e-5
    cost_currency: "CNY"
  srt-llama2: # llama2 served using SRT
    model: default
    provider: api
    is_custom_llm: true
    base_url: "http://127.0.0.1:30000/v1" # the example address of the SRT server
